# offLLM Symbiosis Deep Report (v6)

- Generated: **2025-12-25T03:57:54Z**
- Repo root: `/home/ales27pm/offLLM-1`
- Repo fingerprint: `git:fb949bd0f181a4a0a2f27d594ad4d16ca79c39e6`

## Totals
- **files_seen**: 323
- **text_files_indexed**: 323
- **prompt_signal_files**: 19
- **tool_signal_files**: 29
- **telemetry_signal_files**: 36
- **rag_signal_files**: 45
- **eval_signal_files**: 65
- **ios_signal_files**: 63

## Where to search

### Prompts
- `ios/MyOfflineLLMApp/Turbo/LLM.swift`
- `ios/MyOfflineLLMApp/MLX/MLXModule.swift`
- `.vscode/PythonImportHelper-v2-Completion.json`
- `__tests__/promptBuilder.test.js`
- `src/core/AgentOrchestrator.js`
- `src/utils/buildPrompt.ts`
- `src/services/treeOfThought.js`
- `scripts/ci/download-mlx-model.sh`
- `scripts/train_lora.py`
- `src/native/mlx.ts`
- `src/native/MLXModule.ts`
- `scripts/eval/run_prompt_regression.py`
- `docs/codebase-audit.md`
- `MLXModule.swift`
- `__tests__/evalSummary.test.js`
- `src/services/chat/mlxChat.ts`
- `src/hooks/useMlxChat.ts`
- `prompts/v1/runtime_prompt.json`
- `prompts/v1/training_prompt.json`

### Tools Orchestration
- `.vscode/PythonImportHelper-v2-Completion.json`
- `docs/agent-architecture.md`
- `src/core/tools/ToolHandler.js`
- `__tests__/toolRegistry.test.js`
- `package-lock.json`
- `__tests__/toolHandler.test.js`
- `scripts/mlops/telemetry_redaction.py`
- `scripts/eval/golden_prompts.json`
- `src/App.js`
- `src/utils/telemetry.js`
- `__tests__/goldenPrompts.test.js`
- `scripts/mlops/telemetry_to_sft.py`
- `src/core/AgentOrchestrator.js`
- `__tests__/telemetryDatasets.test.js`
- `src/core/tools/ToolRegistry.js`
- `src/core/prompt/PromptBuilder.js`
- `eval/redteam_tool_injection.json`
- `AGENTS.md`
- `src/core/AGENTS.md`
- `src/tools/AGENTS.md`
- `scripts/eval/run_prompt_regression.py`
- `__tests__/promptBuilder.test.js`
- `scripts/train_lora.py`
- `__tests__/telemetry.test.js`
- `schemas/telemetry_event.schema.json`
- … and 4 more

### Telemetry
- `.vscode/PythonImportHelper-v2-Completion.json`
- `package-lock.json`
- `docs/telemetry-events.md`
- `scripts/eval/write_eval_summary.py`
- `src/utils/telemetry.js`
- `docs/agent-architecture.md`
- `scripts/mlops/telemetry_to_retrieval_triples.py`
- `__tests__/telemetryDatasets.test.js`
- `scripts/mlops/telemetry_to_sft.py`
- `scripts/mlops/telemetry_to_tool_calls.py`
- `scripts/mlops/telemetry_redaction.py`
- `__tests__/evalSummary.test.js`
- `scripts/mlops/generate_retrieval_pairs.py`
- `scripts/eval/retrieval_eval.py`
- `scripts/eval/export_equivalence.py`
- `android/app/src/main/java/com/mongars/LlamaTurboModule.java`
- `__tests__/telemetry.test.js`
- `src/architecture/AGENTS.md`
- `ios/MyOfflineLLMApp/Turbo/LLM.swift`
- `scripts/offllm_end_to_end_pipeline.py`
- `src/services/treeOfThought.js`
- `AGENTS.md`
- `.github/workflows/offllm_pipeline.yml`
- `docs/codebase-audit.md`
- `src/services/AGENTS.md`
- … and 10 more

### Retrieval Rag
- `.vscode/PythonImportHelper-v2-Completion.json`
- `src/utils/hnswVectorStore.js`
- `scripts/offllm_end_to_end_pipeline.py`
- `android/app/src/main/cpp/llama_jni.cpp`
- `docs/agent-architecture.md`
- `src/utils/sparseAttention.js`
- `scripts/eval/write_eval_summary.py`
- `src/services/contextEngineer.js`
- `scripts/eval/golden_prompts.json`
- `__tests__/hnswVectorStore.test.js`
- `ios/MyOfflineLLMApp/Turbo/LLM.swift`
- `scripts/mlops/telemetry_to_retrieval_triples.py`
- `eval/retrieval_eval.py`
- `__tests__/contextEngineer.test.js`
- `__tests__/evalSummary.test.js`
- `__tests__/vectorMemory.test.js`
- `src/utils/vectorUtils.js`
- `android/app/src/main/java/com/mongars/LlamaTurboModule.java`
- `src/core/AGENTS.md`
- `src/core/memory/services/VectorIndexer.js`
- `src/services/AGENTS.md`
- `docs/telemetry-events.md`
- `src/memory/AGENTS.md`
- `scripts/mlops/llm2vec_train.py`
- `src/core/AgentOrchestrator.js`
- … and 10 more

### Evaluation
- `.vscode/PythonImportHelper-v2-Completion.json`
- `__tests__/AGENTS.md`
- `package-lock.json`
- `.github/workflows/prompt_regression.yml`
- `__tests__/toolHandler.test.js`
- `__tests__/telemetry.test.js`
- `src/services/AGENTS.md`
- `docs/agent-architecture.md`
- `.github/workflows/e2e.yml`
- `src/core/AGENTS.md`
- `src/services/treeOfThought.js`
- `AGENTS.md`
- `scripts/eval/write_eval_summary.py`
- `docs/AGENTS.md`
- `scripts/eval/run_prompt_regression.py`
- `__tests__/machoFatBinary.test.js`
- `src/core/tools/ToolHandler.js`
- `__tests__/buildReport.test.js`
- `__tests__/contextEngineer.test.js`
- `__tests__/evalSummary.test.js`
- `__tests__/vectorMemory.test.js`
- `ci/run_prompt_regression.sh`
- `__tests__/promptBuilder.test.js`
- `package.json`
- `docs/codebase-audit.md`
- … and 10 more

### Ios Mlx Coreml
- `.vscode/PythonImportHelper-v2-Completion.json`
- `project.yml`
- `scripts/ci/check_mlx_bridge.js`
- `scripts/ci/download-mlx-model.sh`
- `.github/workflows/offllm_pipeline.yml`
- `ios/project.yml`
- `scripts/offllm_end_to_end_pipeline.py`
- `.github/workflows/ios-deploy-testflight.yml`
- `.github/workflows/ios-build-signed-ipa.yml`
- `README.md`
- `__tests__/hfRepoDownloader.test.js`
- `ios/scripts/patch-mlx-metal-cxx17.sh`
- `scripts/detect_mlx_symbols.sh`
- `src/App.js`
- `scripts/dev/doctor.sh`
- `ios/MyOfflineLLMApp/Turbo/LLM.swift`
- `ios/MyOfflineLLMApp/MLX/MLXModule.swift`
- `scripts/build-ios-unsigned.sh`
- `scripts/apply_ios_mlx_fixes.sh`
- `.github/workflows/ios-signed-monGARS.yml`
- `scripts/ci/bootstrap_ios.sh`
- `ios/Podfile`
- `scripts/convert_to_coreml.py`
- `scripts/ci/select_xcode_and_ensure_ios.sh`
- `docs/agent-architecture.md`
- … and 10 more

### Todos Fixmes

### Hot Churn

## Prompt drift clusters

### Cluster `83875f3f535c1be2` (1 snippets, 1 files)
- marker-window@52-59: console.warn(`[Agent] Pruning context (${currentLength} chars)...`);      // Always keep System Prompt (usually first message)     const systemPrompts = context.filter(       (m) => m.role === "system" && !m.content.startsWith("Observation"
- `src/core/AgentOrchestrator.js`

### Cluster `8d1eaffc23c7ebb5` (1 snippets, 1 files)
- marker-window@13679-13686: "importPath": "scripts.offllm_symbiosis_advisor_v4",         "description": "scripts.offllm_symbiosis_advisor_v4",         "peekOfCode": "PROMPT_MARKERS = [\n    \"SYSTEM_PROMPT\", \"system prompt\", \"You are\", \"### Instruction\", \"### 
- `.vscode/PythonImportHelper-v2-Completion.json`

### Cluster `9f324b92e6802daf` (1 snippets, 1 files)
- marker-window@12914-12921: "importPath": "scripts.mlops.offllm_symbiosis_advisor_v4",         "description": "scripts.mlops.offllm_symbiosis_advisor_v4",         "peekOfCode": "PROMPT_MARKERS = [\n    \"SYSTEM_PROMPT\",\n    \"system prompt\",\n    \"You are\",\n    
- `.vscode/PythonImportHelper-v2-Completion.json`

### Cluster `d874e8da8d0fd1df` (1 snippets, 1 files)
- marker-window@28-35: ### Prompt routing and tool execution  - `PromptBuilder` normalises tool metadata before injecting it into the system prompt so downstream parsing keeps deterministic ordering and excludes malformed entries.【F:src/core/prompt/PromptBuilder.
- `docs/codebase-audit.md`

## Action items

### Unify prompt surfaces into versioned templates (high)
**Why:** Prompt drift breaks alignment between runtime, fine-tuning, and eval.

**Next steps:**
- Create prompts/v1/*.json and a registry.json (id+version).
- Load prompts at runtime by id+version; log prompt_id+version into telemetry.
- Add CI lint: fail if new system prompts are added outside registry.

**Where:**
- `ios/MyOfflineLLMApp/Turbo/LLM.swift`
- `ios/MyOfflineLLMApp/MLX/MLXModule.swift`
- `.vscode/PythonImportHelper-v2-Completion.json`
- `__tests__/promptBuilder.test.js`
- `src/core/AgentOrchestrator.js`
- `src/utils/buildPrompt.ts`
- `src/services/treeOfThought.js`
- `scripts/ci/download-mlx-model.sh`
- `scripts/train_lora.py`
- `src/native/mlx.ts`
- `src/native/MLXModule.ts`
- `scripts/eval/run_prompt_regression.py`
- `docs/codebase-audit.md`
- `MLXModule.swift`
- `__tests__/evalSummary.test.js`

### Standardise telemetry schema and redaction (high)
**Why:** Telemetry is the bridge between what the app did and what the model should learn.

**Next steps:**
- Define an event schema for model interactions.
- Centralise PII redaction (emails, tokens, keys).
- Implement telemetry→SFT and telemetry→retrieval pairs transforms.

**Where:**
- `.vscode/PythonImportHelper-v2-Completion.json`
- `package-lock.json`
- `docs/telemetry-events.md`
- `scripts/eval/write_eval_summary.py`
- `src/utils/telemetry.js`
- `docs/agent-architecture.md`
- `scripts/mlops/telemetry_to_retrieval_triples.py`
- `__tests__/telemetryDatasets.test.js`
- `scripts/mlops/telemetry_to_sft.py`
- `scripts/mlops/telemetry_to_tool_calls.py`
- `scripts/mlops/telemetry_redaction.py`
- `__tests__/evalSummary.test.js`
- `scripts/mlops/generate_retrieval_pairs.py`
- `scripts/eval/retrieval_eval.py`
- `scripts/eval/export_equivalence.py`

### Isolate retrieval + chunking into a single library surface (med)
**Why:** Stable chunking/embedding settings prevent offline vs runtime mismatch.

**Next steps:**
- Extract chunking rules into one module with golden tests.
- Log retrieval traces into telemetry.
- Train embeddings/LLM2Vec with the same chunk distribution used at runtime.

**Where:**
- `.vscode/PythonImportHelper-v2-Completion.json`
- `src/utils/hnswVectorStore.js`
- `scripts/offllm_end_to_end_pipeline.py`
- `android/app/src/main/cpp/llama_jni.cpp`
- `docs/agent-architecture.md`
- `src/utils/sparseAttention.js`
- `scripts/eval/write_eval_summary.py`
- `src/services/contextEngineer.js`
- `scripts/eval/golden_prompts.json`
- `__tests__/hnswVectorStore.test.js`
- `ios/MyOfflineLLMApp/Turbo/LLM.swift`
- `scripts/mlops/telemetry_to_retrieval_triples.py`
- `eval/retrieval_eval.py`
- `__tests__/contextEngineer.test.js`
- `__tests__/evalSummary.test.js`

### Make evaluation first-class (golden set + regression gates) (high)
**Why:** Fine-tuning without a regression gate is just vibe-training.

**Next steps:**
- Create a golden eval suite: tool parsing, JSON validity, groundedness/citations, refusal correctness.
- Add an offline eval CLI and a CI job that blocks regressions.
- Version eval cases alongside prompt templates.

**Where:**
- `.vscode/PythonImportHelper-v2-Completion.json`
- `__tests__/AGENTS.md`
- `package-lock.json`
- `.github/workflows/prompt_regression.yml`
- `__tests__/toolHandler.test.js`
- `__tests__/telemetry.test.js`
- `src/services/AGENTS.md`
- `docs/agent-architecture.md`
- `.github/workflows/e2e.yml`
- `src/core/AGENTS.md`
- `src/services/treeOfThought.js`
- `AGENTS.md`
- `scripts/eval/write_eval_summary.py`
- `docs/AGENTS.md`
- `scripts/eval/run_prompt_regression.py`

### Harden tool-calling boundaries and injection resistance (med)
**Why:** Tool-calling is the highest-risk surface; harden and train for safe behaviour.

**Next steps:**
- Validate tool args against JSON schema before execution.
- Capability-based allowlists.
- Add red-team eval set: injection, schema smuggling, exfil attempts.

**Where:**
- `.vscode/PythonImportHelper-v2-Completion.json`
- `docs/agent-architecture.md`
- `src/core/tools/ToolHandler.js`
- `__tests__/toolRegistry.test.js`
- `package-lock.json`
- `__tests__/toolHandler.test.js`
- `scripts/mlops/telemetry_redaction.py`
- `scripts/eval/golden_prompts.json`
- `src/App.js`
- `src/utils/telemetry.js`
- `__tests__/goldenPrompts.test.js`
- `scripts/mlops/telemetry_to_sft.py`
- `src/core/AgentOrchestrator.js`
- `__tests__/telemetryDatasets.test.js`
- `src/core/tools/ToolRegistry.js`

## Prompt snippet index (sample)
- `docs/codebase-audit.md`:28-35 [marker-window] ### Prompt routing and tool execution  - `PromptBuilder` normalises tool metadata before injecting it into the system prompt so downstream parsing keeps deterministic ordering and excludes malformed entries.【F:src/core/prompt/PromptBuilder.
- `src/core/AgentOrchestrator.js`:52-59 [marker-window] console.warn(`[Agent] Pruning context (${currentLength} chars)...`);      // Always keep System Prompt (usually first message)     const systemPrompts = context.filter(       (m) => m.role === "system" && !m.content.startsWith("Observation"
- `.vscode/PythonImportHelper-v2-Completion.json`:12914-12921 [marker-window] "importPath": "scripts.mlops.offllm_symbiosis_advisor_v4",         "description": "scripts.mlops.offllm_symbiosis_advisor_v4",         "peekOfCode": "PROMPT_MARKERS = [\n    \"SYSTEM_PROMPT\",\n    \"system prompt\",\n    \"You are\",\n    
- `.vscode/PythonImportHelper-v2-Completion.json`:13679-13686 [marker-window] "importPath": "scripts.offllm_symbiosis_advisor_v4",         "description": "scripts.offllm_symbiosis_advisor_v4",         "peekOfCode": "PROMPT_MARKERS = [\n    \"SYSTEM_PROMPT\", \"system prompt\", \"You are\", \"### Instruction\", \"### 
